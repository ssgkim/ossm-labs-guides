:tutorial-url: https://github.com/redhat-scholars/istio-tutorial
:folder: istio-tutorial

= Deploy Microservices
include::_attributes.adoc[]
이 섹션에서는 튜토리얼에서 사용된 세 가지 마이크로서비스의 배포 방법을 안내합니다.

각 마이크로서비스는 `tutorial` 네임스페이스에 배포되며, [Deployment](https://kubernetes.io/docs/concepts/workloads/controllers/deployment/)를 사용하여 관리됩니다. 이 배포는 다음과 같은 주석을 포함합니다:

[.console-output]
[source,text,subs="attributes+,+macros"]
----
sidecar.istio.io/inject: "true"
----

이 주석은 `tutorial` 네임스페이스가 메쉬의 일부로 포함되어 있고 (*Setup* 섹션에서 생성한 *ServiceMeshMemberRoll* 덕분에) 각 마이크로서비스가 트래픽을 관리할 수 있는 Istio "사이드카"를 갖게 됨을 의미합니다. Istio 문서를 참조하여 [트래픽 관리 및 사이드카(Envoy 기반) 프록시](https://istio.io/latest/docs/concepts/traffic-management/)에 대해 자세히 알아보세요.

image:architecture-basic.png[]

ifdef::workshop[]
사용자 환경 변수를 생성하고 OpenShift에 로그인하세요.


[source,bash,subs="+macros,+attributes"]
----
export WORKSHOP_USER=<your-username-number>


#Example:
export WORKSHOP_USER=1

oc login -u user$WORKSHOP_USER -p openshift {ocpurl}
----
endif::workshop[]

== 로그인, 네임스페이스 및 튜토리얼 파일 확인

[tabs, subs="attributes+,+macros"]  
--
OpenShift (웹 터미널)::
+
--

`Setup & Installation` xref:1setup.adoc#istioinstallation[섹션]에서 OpenShift Web Terminal을 설치했다면, OpenShift Web 콘솔을 새로 고침하고 화면 오른쪽 상단의 터미널 아이콘을 클릭하여 웹 터미널을 시작합니다.

image:deploy-web-terminal.png[OpenShift Web Terminal]

터미널 세션은 `~/.kube/config` 파일로 미리 구성되어 있습니다. 이는 `oc`와 `kubectl`이 이미 로그인된 상태임을 의미합니다. 다음 명령어를 실행하여 확인하세요:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc whoami
----

이 명령어는 사용자의 사용자 이름을 출력합니다. `kubectl` 을 사용하여 확인할 수도 있습니다:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl config current-context
----

다음 명령어를 실행하여 `tutorial` 네임스페이스를 생성하고 현재 컨텍스트로 설정합니다:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc new-project tutorial ; oc project tutorial
----

// 이 파일 상단에 선언된 `tutorial-url` 및 `folder` 변수 사용
include::https://raw.githubusercontent.com/redhat-developer-demos/rhd-tutorial-common/master/download-sources.adoc[]

[IMPORTANT]
====
OpenShift Web Terminal은 지속적인 환경이 아닙니다. OpenShift Web Terminal이 닫히거나 몇 분간 비활성화되면 `istio-tutorial` 폴더, `TUTORIAL_HOME`, 및 설정된 환경 변수는 손실됩니다. Web Terminal 세션을 다시 구성해야 할 경우 이 섹션으로 돌아가서 `git clone`, `export`, 및 `cd` 명령어를 다시 실행하세요.
====


--
OpenShift (Local Terminal)::
+
--
1. OpenShift Web Console의 오른쪽 상단에 있는 (*?* 아이콘) 도움말 메뉴에서 OpenShift CLI (`oc`)를 다운로드하고 *Copy login command* 링크를 확인합니다.
+
image:deploy-openshift-cli.png[OpenShift Help and Login Command]

2. CLI를 압축 해제하고 `PATH`에 추가합니다.
3. CLI를 사용하여 로그인하려면 *Copy login command* 의 값을 터미널에 붙여넣습니다.
+
image:deploy-openshift-cli-local.png[Using OpenShift CLI and kubectl locally]

4. 다음 명령어를 실행하여 `tutorial` 네임스페이스를 생성하고 현재 컨텍스트로 설정합니다:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc new-project tutorial ; oc project tutorial
----



[#deploycustomer]

== 고객 서비스 배포

[NOTE]
====
이 튜토리얼을 위해 이전에 빌드된 컨테이너 이미지를 배포합니다. 자신만의 버전의 이미지를 배포할 수도 있습니다.

이 서비스를 위한 맞춤형 컨테이너 이미지를 빌드하고 배포하려면 *Quarkus* 를 사용하는 경우 xref:2build-microservices.adoc#buildcustomer[여기를 클릭하세요]. *Spring Boot* 를 사용하고자 한다면 xref:2build-microservices.adoc#buildcustomerspringboot[여기를 클릭하세요].
====

[IMPORTANT]
====
컨테이너 이미지의 맞춤 버전을 빌드하기로 선택한 경우, 이후 단계에서 참조되는 *Deployment.yml* 파일의 `image` 를 수정하는 것을 잊지 마세요.
====

고객 애플리케이션을 https://github.com/redhat-scholars/istio-tutorial[tutorial content]에 제공된 YAML 파일을 사용하여 배포합니다:


[tabs, subs="attributes+,+macros"]	
====
--
OpenShift::
+
--
. *Deployment.yml* 파일을 사용하여 고객 애플리케이션을 배포합니다. 이 배포에는 `sidecar.istio.io/inject: "true"` 주석이 포함되어 있으며, 이는 Istio가 네트워크 트래픽을 관리하기 위해 사이드카 컨테이너를 주입함을 의미합니다:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f link:{github-repo}/{customer-repo}/kubernetes/Deployment.yml[{customer-repo}/kubernetes/Deployment.yml] -n tutorial
----

. 고객 애플리케이션의 Pod가 클러스터에서 안정적인 DNS 항목을 갖도록 Service를 생성합니다:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl create -f link:{github-repo}/{customer-repo}/kubernetes/Service.yml[{customer-repo}/kubernetes/Service.yml] -n tutorial
----

. 고객 애플리케이션 Pod가 시작되었고 모든 컨테이너가 정상적으로 보고되고 있는지 확인합니다.
`READY`:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl get pods -w -n tutorial
----

--
====

`get pods` 명령의 출력 결과는 최종적으로 2/2 컨테이너가 READY 상태에 있는 것을 다음과 같이 보여줍니다:


[.console-output]
[source,bash,subs="+macros,+attributes"]
----
NAME                        READY   STATUS    RESTARTS   AGE
customer-5f5d9f8767-dmc4f   2/2     Running   0          5m24s
----

Ctrl+C를 눌러 모든 컨테이너가 READY 상태로 보고될 때 `watch (-w)` 명령을 종료합니다.

*Deployment.yml* 의 `spec.replicas` 는 1로 설정되어 있는데, 왜 `get pods` 명령에서 2개의 컨테이너가 READY 상태로 표시될까요?  
그 이유는 Istio가 네트워크 트래픽을 관리하기 위해 사이드카 컨테이너를 주입했기 때문입니다!

`kubectl describe pods -n tutorial` 명령을 사용하여 고객 애플리케이션의 Pod을 확인할 수 있습니다. 이 명령을 통해 Istio 주석과 Istio 오퍼레이터에 의해 Pod 스펙에 자동으로 추가된 두 번째 컨테이너를 확인할 수 있습니다.

[#configureingress]
== 고객 서비스의 인그레스(Ingress) 구성

`customer` 서비스는 사용자가 상호 작용하는 서비스이므로, 애플리케이션으로 들어오는 트래픽을 전달할 수 있도록 https://istio.io/latest/docs/reference/config/networking/gateway/[Gateway] 및 https://istio.io/latest/docs/reference/config/networking/virtual-service/[VirtualService]를 생성합니다.

Gateway 리소스는 메쉬의 가장자리에서 들어오는 TCP/HTTP 트래픽을 수신하기 위한 로드 밸런서를 구성합니다. VirtualService는 특정 URL 패턴과 일치하는 트래픽을 고객 서비스로 보내기 위한 트래픽 라우팅 규칙을 정의합니다.


image:architecture-basic-gateway-virtualservice.png[]

ifndef::workshop[]

Gateway 및 VirtualService를 다음 명령어를 사용하여 배포합니다:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
cat << EOF > Gateway.yaml
apiVersion: networking.istio.io/v1alpha3
kind: VirtualService
metadata:
  name: customer-gateway
spec:
  hosts:
  - "*"
  gateways:
  - user1-dev-istio-system/control-gateway
  http:
  - match:
    - uri:
        prefix: /customer
    rewrite:
      uri: /
    route:
    - destination:
        host: customer
        port:
          number: 8080
EOF

oc create -f Gateway.yaml -n tutorial
----

Gateway 및 VirtualService는 Istio 트래픽 관리를 구성하는 데 사용되는 논리적 개체입니다. 메쉬로 들어오는 모든 트래픽은 `user1-dev-istio-system` 네임스페이스에 배포된 `istio-ingressgateway` 를 통해 유입됩니다. `istio-ingressgateway` 구성 요소가 배포되었는지 확인하려면 다음 명령어를 사용하세요:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
oc get all -l app=istio-ingressgateway -n user1-dev-istio-system
----

위 명령어의 출력은 OpenShift 환경에 따라 약간 다를 수 있습니다.

[tabs, subs="attributes+,+macros"]	
====
--
OpenShift::
+
--

[.console-output]
[source,bash,subs="attributes+,+macros"]
----
NAME                                        READY   STATUS    RESTARTS   AGE
pod/istio-ingressgateway-6f7f4b8778-7s7zg   1/1     Running   0          175m

NAME                           TYPE        CLUSTER-IP    EXTERNAL-IP   PORT(S)                    AGE
service/istio-ingressgateway   ClusterIP   10.217.4.72   <none>        15021/TCP,80/TCP,443/TCP   175m

NAME                                   READY   UP-TO-DATE   AVAILABLE   AGE
deployment.apps/istio-ingressgateway   1/1     1            1           175m

NAME                                              DESIRED   CURRENT   READY   AGE
replicaset.apps/istio-ingressgateway-6f7f4b8778   1         1         1       175m

NAME                                            HOST/PORT                                            PATH   SERVICES               PORT   TERMINATION   WILDCARD
route.route.openshift.io/istio-ingressgateway   istio-ingressgateway-istio-system.apps-crc.testing          istio-ingressgateway   8080                 None
----

--
====

== 인그레스(Ingress) 검증

VirtualService를 사용하여 트래픽 인그레스 및 라우팅이 정상적으로 작동하는지 확인하려면 고객 애플리케이션에 HTTP 요청을 보냅니다.

=== 인그레스 URL 가져오기

인그레스 URL을 가져와 터미널의 `GATEWAY_URL` 변수에 저장합니다.

[tabs]
====
--
OpenShift::
+
--
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
export GATEWAY_URL=$(kubectl get route istio-ingressgateway -n user1-dev-istio-system -o=jsonpath="{.spec.host}")
----
--
====

[IMPORTANT]
====
`GATEWAY_URL`은 이 가이드 전반에 걸쳐 자주 사용됩니다. 이 값을 잃어버린 경우 다시 이 위치로 돌아와 위 명령어를 실행하여 다시 가져올 수 있습니다. URL을 출력하려면 다음 명령어를 사용하세요:
+
[source,bash]
----
echo $GATEWAY_URL
----
====


endif::workshop[]

ifdef::workshop[]
[source,bash,subs="+macros,+attributes"]
----
envsubst < link:{github-repo}/{customer-repo}/kubernetes/Gateway.workshop.yml[{customer-repo}/kubernetes/Gateway.workshop.yml] | oc create -f - -n tutorial
or
envsubst < link:{github-repo}/{customer-repo}/kubernetes/Gateway.workshop.yml[{customer-repo}/kubernetes/Gateway.workshop.yml] | kubectl create -f - -n tutorial

oc get pods -w -n tutorial
or
kubectl get pods -w -n tutorial
----
endif::workshop[]
[IMPORTANT]
====
만약 포드가 `ImagePullBackOff` 오류로 실패하는 경우, 현재 터미널이 올바른 Docker 환경을 사용하지 않을 수 있습니다. 환경 설정에 대한 자세한 내용은 link:#setup-environment[환경 설정]을 참조하십시오.
====

포드의 상태가 `Running`으로 표시되고 `Ready` 열에 `2/2` 포드가 표시될 때까지 기다립니다. 작업을 종료하려면 `Ctrl+C` 를 누르세요.

=== cURL을 사용하여 인그레스(Ingress) 테스트하기

cURL을 사용하여 고객 애플리케이션의 엔드포인트를 테스트합니다:

include::curl.adoc[]

다음과 같은 응답이 반환되어야 합니다. `preference` 및 `recommendation` 애플리케이션이 아직 배포되지 않았기 때문에 응답에 `UnknownHostException`이 포함됩니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
customer => UnknownHostException: preference
----

=== 고객 애플리케이션 로그 검토

다음 명령어는 `customer` 컨테이너의 로그를 반환하지만, Pod 내의 `istio-proxy` 사이드카 컨테이너의 로그는 반환하지 않습니다.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl logs \
$(kubectl get pods -n tutorial |grep customer|awk '{ print $1 }'|head -1) \
-c customer -n tutorial
----

로그에 cURL 명령어에서 보고된 `UnknownHostException`이 포함된 스택 트레이스가 표시되어야 합니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
customer-6fc99b7bfd-5st28 customer Caused by: java.net.UnknownHostException: preference
----

[#deploypreference]
== Preference 서비스 배포

[NOTE]
====
이 튜토리얼을 위해 이전에 빌드된 컨테이너 이미지를 배포합니다. 자신만의 버전의 이미지를 배포할 수도 있습니다.

이 서비스를 위한 맞춤형 컨테이너 이미지를 빌드하고 배포하려면 *Quarkus* 를 사용하는 경우 xref:2build-microservices.adoc#buildpreference[여기를 클릭하세요]. *Spring Boot* 를 사용하고자 한다면 xref:2build-microservices.adoc#buildpreferencespringboot[여기를 클릭하세요].
====

[IMPORTANT]
====
컨테이너 이미지의 맞춤 버전을 빌드하기로 선택한 경우, 이후 단계에서 참조되는 *Deployment.yml* 파일의 `image` 를 수정하는 것을 잊지 마세요.
====
====

=== Preference 서비스 리소스 적용

[tabs, subs="attributes+,+macros"]  
====
--
OpenShift::
+
--

Preference 애플리케이션의 배포 프로세스는 고객 애플리케이션의 배포 프로세스와 동일합니다. Preference의 *Deployment.yml* 파일에도 `sidecar.istio.io/inject: "true"` 주석이 포함되어 있습니다.

. *Deployment.yml* 파일을 사용하여 Preference 애플리케이션을 배포합니다:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f link:{github-repo}/{preference-repo}/kubernetes/Deployment.yml[{preference-repo}/kubernetes/Deployment.yml] -n tutorial
----

. Preference 애플리케이션에 안정적인 DNS 항목을 제공하기 위해 Service를 생성합니다:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl create -f link:{github-repo}/{preference-repo}/kubernetes/Service.yml[{preference-repo}/kubernetes/Service.yml] -n tutorial
----

--
====

Preference 애플리케이션 Pod가 시작되었는지 확인합니다. 모든 컨테이너의 상태가 `READY` 로 표시되면 `watch (-w)` 명령을 종료하려면 `Ctrl+C` 를 누릅니다.

[.console-input]
[source, bash,subs="+macros,+attributes"]
----
kubectl get pods -w -n tutorial
----

=== Preference 서비스 연결 확인

이제 Preference 서비스가 배포되었으므로, 고객 서비스는 들어오는 HTTP 요청에 대해 다른 응답을 반환해야 합니다. cURL을 사용하여 이를 확인합니다.

include::curl.adoc[]

응답에는 여전히 `UnknownHostException`이 표시되지만, 이번에는 `recommendation` 서비스와 관련이 있습니다. 이는 `recommendation` 서비스가 아직 배포되지 않았기 때문입니다.

// NOTE: 튜토리얼의 향후 버전에서는 이 부분을 더 견고하게 만들 수 있습니다.


[.console-output]
[source,bash,subs="+macros,+attributes"]
----
customer => Error: 503 - preference => UnknownHostException: recommendation
----

=== Preference 애플리케이션 로그 검토

다음 명령어를 사용하여 Preference 애플리케이션의 로그를 확인합니다:

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl logs \
$(kubectl get pods -n tutorial |grep preference|awk '{ print $1 }'|head -1) \
-c preference -n tutorial
----

cURL 명령어에서 보고된 `UnknownHostException`이 포함된 스택 트레이스가 로그에 표시되어야 합니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
preference-v1-898764bdb-hz7s6 preference Caused by: java.net.UnknownHostException: recommendation
----

[#deployrecommendation]
== Recommendation 서비스 배포

[NOTE]
====
이 튜토리얼을 위해 이전에 빌드된 컨테이너 이미지를 배포합니다. 자신만의 버전의 이미지를 배포할 수도 있습니다.

이 서비스를 위한 맞춤형 컨테이너 이미지를 빌드하고 배포하려면 *Quarkus* 를 사용하는 경우 xref:2build-microservices.adoc#buildrecommendation[여기를 클릭하세요]. *Spring Boot* 를 사용하고자 한다면 xref:2build-microservices.adoc#buildrecommendationspringboot[여기를 클릭하세요].
====

[IMPORTANT]
====
컨테이너 이미지의 맞춤 버전을 빌드하기로 선택한 경우, 이후 단계에서 참조되는 *Deployment.yml* 파일의 `image` 를 수정하는 것을 잊지 마세요.
====

=== Recommendation 서비스 리소스 적용

[tabs, subs="attributes+,+macros"]  
====
--
OpenShift::
+
--

Recommendation 애플리케이션의 배포 프로세스는 고객 애플리케이션의 배포 프로세스와 동일합니다. Recommendation의 *Deployment.yml* 파일에도 `sidecar.istio.io/inject: "true"` 주석이 포함되어 있습니다.

. *Deployment.yml* 파일을 사용하여 Recommendation 애플리케이션을 배포합니다:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl apply -f link:{github-repo}/{recommendation-repo}/kubernetes/Deployment.yml[{recommendation-repo}/kubernetes/Deployment.yml] -n tutorial
----

. Recommendation 애플리케이션에 안정적인 DNS 항목을 제공하기 위해 Service를 생성합니다:
+
[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl create -f link:{github-repo}/{recommendation-repo}/kubernetes/Service.yml[{recommendation-repo}/kubernetes/Service.yml] -n tutorial
----

--
====

Recommendation 애플리케이션 Pod가 시작되었는지 확인합니다. 모든 컨테이너의 상태가 `READY` 로 표시되면 `watch (-w)` 명령을 종료하려면 `Ctrl+C` 를 누릅니다.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl get pods -w -n tutorial
----


[.console-input]
[source, bash,subs="+macros,+attributes"]
----
kubectl get pods -w -n tutorial
----

=== Recommendation 서비스 연결 확인

이제 Recommendation 서비스가 배포되었으므로, 고객 서비스는 들어오는 HTTP 요청에 대해 다른 응답을 반환해야 합니다. cURL을 사용하여 이를 확인합니다.

include::curl.adoc[]

서비스의 엔드 투 엔드(end-to-end) 플로우가 모두 배포되었으므로 응답에 오류가 포함되지 않습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
customer => preference => recommendation v1 from 'recommendation-v1-6cf5ff55d9-7zbj8': 1
----

=== Recommendation 애플리케이션 로그 검토

다음 명령어를 사용하여 Recommendation 애플리케이션의 로그를 확인합니다.

[.console-input]
[source,bash,subs="+macros,+attributes"]
----
kubectl logs \
$(kubectl get pods -n tutorial |grep recommendation|awk '{ print $1 }'|head -1) \
-c recommendation -n tutorial
----

로그에는 오류가 보고되지 않아야 하며, 대신 증가하는 카운터를 볼 수 있습니다.

[.console-output]
[source,bash,subs="+macros,+attributes"]
----
recommendation request from recommendation-v1-6c75fc9857-d4npl: 1
----
